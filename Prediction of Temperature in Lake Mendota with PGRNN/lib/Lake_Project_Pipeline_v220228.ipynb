{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, random, warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import torch\n",
    "# import cv2\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import sciencebasepy\n",
    "import urllib.request\n",
    "from sklearn import preprocessing\n",
    "from urllib.request import urlopen\n",
    "from zipfile import ZipFile\n",
    "from convlstm import ConvLSTM, ConvLSTMCell\n",
    "\n",
    "def download_data(data_dir):\n",
    "    if os.listdir(data_dir) == []: ## data not downloaded yet\n",
    "        print('Data folder is empty! Download the files now!')\n",
    "        # set the url\n",
    "        zipurl = 'https://github.com/leap-stc/LEAPCourse-Climate-Pred-Challenges/raw/main/Project-StarterCodes/Project2-PhysicsML/data/numpy_files.zip'\n",
    "        # download the file from the URL\n",
    "        zipresp = urlopen(zipurl)\n",
    "        # create a new file on the hard drive\n",
    "        tempzip = open(data_dir + 'numpy_files.zip', \"wb\")\n",
    "        # write the contents of the downloaded file into the new file\n",
    "        tempzip.write(zipresp.read())\n",
    "        # close the newly-created file\n",
    "        tempzip.close()\n",
    "        # re-open the newly-created file with ZipFile()\n",
    "        zf = ZipFile(data_dir + 'numpy_files.zip')\n",
    "        # extract its contents into <extraction_path>\n",
    "        # note that extractall will automatically create the path\n",
    "        zf.extractall(path = data_dir)\n",
    "        # close the ZipFile instance\n",
    "        zf.close()\n",
    "        print('Files all downloaded!')\n",
    "        \n",
    "def read_data(data_dir, simulate = True):\n",
    "    # load data\n",
    "    x_full = np.load(data_dir + '/processed_features.npy') #standardized inputs\n",
    "    x_raw_full = np.load(data_dir + '/features.npy') #raw inputs\n",
    "    if simulate:\n",
    "        diag_full = np.load(data_dir + '/diag.npy') \n",
    "        label_full = np.load(data_dir + '/labels.npy') #simulated lake temperatures\n",
    "\n",
    "        # process data\n",
    "        mask_full = np.ones(label_full.shape) # no missing values to mask for simulated data\n",
    "        phy_full = np.concatenate((x_raw_full[:,:,:(-2)], diag_full), axis=2) \n",
    "        ## phy: 4-air temp, 5-rel hum, 6-wind speed, 9-ice flag\n",
    "    else:\n",
    "        diag_full = np.load(data_dir + 'diag.npy')\n",
    "        label_full = np.load(data_dir + 'Obs_temp.npy') # real observation data\n",
    "        mask_full = np.load(data_dir + 'Obs_mask.npy') # flags of missing values\n",
    "        phy_full = np.concatenate((x_raw_full[:,:,:-2], diag_full), axis = 2) #physics variables\n",
    "    full_dict = {\n",
    "        'x':x_full,\n",
    "        'x_raw': x_raw_full,\n",
    "        'diag': diag_full,\n",
    "        'label':label_full,\n",
    "        'mask':mask_full,\n",
    "        'phy':phy_full\n",
    "    }\n",
    "    return full_dict\n",
    "\n",
    "\n",
    "\n",
    "def data_split(full_dict):\n",
    "    N = full_dict['x'].shape[1]\n",
    "    idx_tr, idx_va, idx_te = (int(N/3), int(N/3*2), N)\n",
    "    \n",
    "    train_dict = {}\n",
    "    valid_dict = {}\n",
    "    test_dict = {}\n",
    "    train_full_dict = {}\n",
    "    for key, val in full_dict.items():\n",
    "        train_dict[key] = val[:, :idx_tr]\n",
    "        valid_dict[key] = val[:, idx_tr:idx_va]\n",
    "        train_full_dict[key] = val[:, :idx_va]\n",
    "        \n",
    "        test_dict[key] = val[:, idx_va:]\n",
    "    return train_dict, valid_dict, train_full_dict, test_dict\n",
    "\n",
    "def generate_samples(dict_, window_size = 352, strides = 352//2,):\n",
    "    sample_dict = {}\n",
    "    for key, val in dict_.items():\n",
    "        sample_dict[key] = []\n",
    "    \n",
    "    size = dict_['x'].shape[1]\n",
    "    for key, val in dict_.items():\n",
    "        loc = 0\n",
    "        while loc + window_size < size:\n",
    "            \n",
    "            tmp_array = val[:, loc: loc + window_size]\n",
    "            tmp_array = np.expand_dims(tmp_array, axis = 0)\n",
    "            sample_dict[key].append(tmp_array)\n",
    "            \n",
    "            loc += strides\n",
    "        sample_dict[key] = np.vstack(sample_dict[key]).astype(np.float32)\n",
    "    return sample_dict\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader, Dataset\n",
    "\n",
    "\n",
    "from typing import List\n",
    "\n",
    "\n",
    "class LakeDataset(Dataset):\n",
    "    def __init__(self, dic):\n",
    "        self.dic = dic\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        tmp_dic = {}\n",
    "        for key, val in self.dic.items():\n",
    "            tmp_dic[key] = val[index, :]\n",
    "        \n",
    "        return tmp_dic\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.dic['x'].shape[0]\n",
    "    \n",
    "    \n",
    "def mask_helper(label, mask, mask_rate = 0.99):\n",
    "    if mask_rate != 0:\n",
    "        depth, days = label.shape\n",
    "        label = label.reshape(-1, 1)\n",
    "        mask = mask.reshape(-1, 1)\n",
    "        idx = np.random.choice(np.arange(label.shape[0]), \n",
    "                               replace=False, \n",
    "                               size=int(label.shape[0] * mask_rate))\n",
    "        label[idx, ] = 0\n",
    "        mask[idx, ] = 0\n",
    "        label = label.reshape(depth, days)\n",
    "        mask = label.reshape(depth, days)\n",
    "    else:\n",
    "        pass\n",
    "    return label, mask\n",
    "\n",
    "def get_dataloader(path, window_size, strides, batch_size, mask_rate = 0.99, simulate = True):\n",
    "    full_dict = read_data(path, simulate = simulate)\n",
    "    train_dict, valid_dict, train_full_dict, test_dict = data_split(full_dict)\n",
    "    \n",
    "    train_dict['label'], train_dict['mask'] = mask_helper(train_dict['label'], train_dict['mask'], mask_rate)\n",
    "    valid_dict['label'], valid_dict['mask'] = mask_helper(valid_dict['label'], valid_dict['mask'], mask_rate)\n",
    "    train_full_dict['label'], train_full_dict['mask'] = mask_helper(train_full_dict['label'], \n",
    "                                                                    train_full_dict['mask'], \n",
    "                                                                    mask_rate)\n",
    "    \n",
    "    train_samples = generate_samples(train_dict, window_size = window_size, strides = strides)\n",
    "    valid_samples = generate_samples(valid_dict, window_size = window_size, strides = strides)\n",
    "    test_samples = generate_samples(test_dict, window_size = window_size, strides = strides)\n",
    "    \n",
    "\n",
    "\n",
    "    train_dataset = LakeDataset(train_samples)\n",
    "    valid_dataset = LakeDataset(valid_samples)\n",
    "    test_dataset = LakeDataset(test_samples)\n",
    "    tr_loader = DataLoader(dataset = train_dataset, batch_size = batch_size, shuffle=True)\n",
    "    va_loader = DataLoader(dataset = valid_dataset, batch_size = batch_size)\n",
    "\n",
    "    te_loader = DataLoader(dataset = test_dataset, batch_size = batch_size)\n",
    "\n",
    "\n",
    "    full_samples = generate_samples(train_full_dict, window_size = window_size, strides = strides)\n",
    "    full_dataset = LakeDataset(full_samples)\n",
    "    full_loader = DataLoader(dataset = train_dataset, batch_size = batch_size)\n",
    "    \n",
    "    return tr_loader, va_loader, full_loader, te_loader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "def density_calculation(temp):\n",
    "    # converts temperature to density\n",
    "    # parameter:\n",
    "        # @temp: single value or array of temperatures to be transformed\n",
    "    densities = 1000 * (1 - ((temp + 288.9414) * (temp - 3.9863)**2) / (508929.2 * (temp + 68.12963)))\n",
    "    return densities\n",
    "\n",
    "def lake_energy_calculation(temps, densities, depth_areas):\n",
    "    # calculate the total energy of the lake for every timestep\n",
    "    # sum over all layers the (depth cross-sectional area)*temp*density*layer_height)\n",
    "    # then multiply by the specific heat of water \n",
    "    dz = 0.5 # thickness for each layer, hardcoded for now\n",
    "    cw = 4186 # specific heat of water\n",
    "    depth_areas = torch.reshape(depth_areas, (-1, 1))\n",
    "    energy = torch.sum(depth_areas * temps * densities * dz * cw, axis=1)\n",
    "    return energy\n",
    "\n",
    "def calculate_lake_energy_deltas(energies, combine_days, surface_area):\n",
    "    # given a time series of energies, compute and return the differences\n",
    "    # between each time step, or time step interval (parameter @combine_days)\n",
    "    # as specified by parameter @combine_days\n",
    "    time = 86400 #seconds per day\n",
    "    energy_deltas = (energies[:,1:] - energies[:,:-1]) / (time * surface_area)\n",
    "    return energy_deltas\n",
    "\n",
    "def calculate_vapour_pressure_saturated(temp):\n",
    "    # returns in miilibars\n",
    "    # Converted pow function to exp function workaround pytorch not having autograd implemented for pow\n",
    "    exponent = (9.28603523 - (2332.37885 / (temp + 273.15))) * np.log(10)\n",
    "    return torch.exp(exponent)\n",
    "\n",
    "def calculate_vapour_pressure_air(rel_hum, temp):\n",
    "    rh_scaling_factor = 1\n",
    "    return rh_scaling_factor * (rel_hum / 100) * calculate_vapour_pressure_saturated(temp)\n",
    "\n",
    "\n",
    "def calculate_wind_speed_10m(ws, ref_height = 2.):\n",
    "    # from GLM code glm_surface.c\n",
    "    c_z0 = torch.tensor(0.001) #default roughness\n",
    "    return ws * (torch.log(10.0 / c_z0) / torch.log(ref_height / c_z0))\n",
    "\n",
    "\n",
    "def calculate_air_density(air_temp, rh):\n",
    "    # returns air density in kg / m^3\n",
    "    # equation from page 13 GLM/GLEON paper(et al Hipsey)\n",
    "    # Ratio of the molecular (or molar) weight of water to dry air\n",
    "    mwrw2a = 18.016 / 28.966\n",
    "    c_gas = 1.0e3 * 8.31436 / 28.966\n",
    "\n",
    "    # atmospheric pressure\n",
    "    p = 1013. #mb\n",
    "\n",
    "    # water vapor pressure\n",
    "    vapPressure = calculate_vapour_pressure_air(rh, air_temp)\n",
    "\n",
    "    # water vapor mixing ratio (from GLM code glm_surface.c)\n",
    "    r = mwrw2a * vapPressure / (p - vapPressure)\n",
    "    return (1.0 / c_gas * (1 + r)/(1 + r / mwrw2a) * p / (air_temp + 273.15)) * 100\n",
    "\n",
    "def calculate_heat_flux_sensible(surf_temp, air_temp, rel_hum, wind_speed):\n",
    "    # equation 22 in GLM/GLEON paper(et al Hipsey)\n",
    "    # GLM code ->  Q_sensibleheat = -CH * (rho_air * 1005.) * WindSp * (Lake[surfLayer].Temp - MetData.AirTemp);\n",
    "    # calculate air density \n",
    "    rho_a = calculate_air_density(air_temp, rel_hum)\n",
    "\n",
    "    # specific heat capacity of air in J/(kg*C)\n",
    "    c_a = 1005.\n",
    "\n",
    "    # bulk aerodynamic coefficient for sensible heat transfer\n",
    "    c_H = 0.0013\n",
    "\n",
    "    # wind speed at 10m\n",
    "    U_10 = calculate_wind_speed_10m(wind_speed)\n",
    "    return -rho_a * c_a * c_H * U_10 * (surf_temp - air_temp)\n",
    "\n",
    "def calculate_heat_flux_latent(surf_temp, air_temp, rel_hum, wind_speed):\n",
    "    # equation 23 in GLM/GLEON paper(et al Hipsey)\n",
    "    # GLM code-> Q_latentheat = -CE * rho_air * Latent_Heat_Evap * (0.622/p_atm) * WindSp * (SatVap_surface - MetData.SatVapDef)\n",
    "    # where,         SatVap_surface = saturated_vapour(Lake[surfLayer].Temp);\n",
    "    #                rho_air = atm_density(p_atm*100.0,MetData.SatVapDef,MetData.AirTemp);\n",
    "    # air density in kg/m^3\n",
    "    rho_a = calculate_air_density(air_temp, rel_hum)\n",
    "\n",
    "    # bulk aerodynamic coefficient for latent heat transfer\n",
    "    c_E = 0.0013\n",
    "\n",
    "    # latent heat of vaporization (J/kg)\n",
    "    lambda_v = 2.453e6\n",
    "\n",
    "    # wind speed at 10m height\n",
    "    # U_10 = wind_speed\n",
    "    U_10 = calculate_wind_speed_10m(wind_speed)\n",
    "    # \n",
    "    # ratio of molecular weight of water to that of dry air\n",
    "    omega = 0.622\n",
    "\n",
    "    # air pressure in mb\n",
    "    p = 1013.\n",
    "\n",
    "    e_s = calculate_vapour_pressure_saturated(surf_temp)\n",
    "    e_a = calculate_vapour_pressure_air(rel_hum, air_temp)\n",
    "    return -rho_a * c_E * lambda_v * U_10 * (omega / p) * (e_s - e_a)\n",
    "\n",
    "def calculate_energy_fluxes(phys, surf_temps, combine_days):    \n",
    "    e_s = 0.985 # emissivity of water, given by Jordan\n",
    "    alpha_sw = 0.07 # shortwave albedo, given by Jordan Read\n",
    "    alpha_lw = 0.03 # longwave, albeda, given by Jordan Read\n",
    "    sigma = 5.67e-8 # Stefan-Baltzmann constant\n",
    "    R_sw_arr = phys[:-1,2] + (phys[1:,2] - phys[:-1,2]) / 2\n",
    "    R_lw_arr = phys[:-1,3] + (phys[1:,3] - phys[:-1,3]) / 2\n",
    "    R_lw_out_arr = e_s * sigma * (torch.pow(surf_temps[:] + 273.15, 4))\n",
    "    R_lw_out_arr = R_lw_out_arr[:-1] + (R_lw_out_arr[1:] - R_lw_out_arr[:-1]) / 2\n",
    "\n",
    "    air_temp = phys[:-1,4] \n",
    "    air_temp2 = phys[1:,4]\n",
    "    rel_hum = phys[:-1,5]\n",
    "    rel_hum2 = phys[1:,5]\n",
    "    ws = phys[:-1, 6]\n",
    "    ws2 = phys[1:,6]\n",
    "    t_s = surf_temps[:-1]\n",
    "    t_s2 = surf_temps[1:]\n",
    "    E = calculate_heat_flux_latent(t_s, air_temp, rel_hum, ws)\n",
    "    H = calculate_heat_flux_sensible(t_s, air_temp, rel_hum, ws)\n",
    "    E2 = calculate_heat_flux_latent(t_s2, air_temp2, rel_hum2, ws2)\n",
    "    H2 = calculate_heat_flux_sensible(t_s2, air_temp2, rel_hum2, ws2)\n",
    "    E = (E + E2) / 2\n",
    "    H = (H + H2) / 2\n",
    "    fluxes = (R_sw_arr[:-1] * (1-alpha_sw) + R_lw_arr[:-1] * (1-alpha_lw) - R_lw_out_arr[:-1] + E[:-1] + H[:-1])\n",
    "    return fluxes\n",
    "\n",
    "\n",
    "def energy_fluxes_calculation(phys, surf_temps):\n",
    "    e_s = 0.985 # emissivity of water, given by Jordan\n",
    "    alpha_sw = 0.07 # shortwave albedo, given by Jordan Read\n",
    "    alpha_lw = 0.03 # longwave, albeda, given by Jordan Read\n",
    "    sigma = 5.67e-8 # Stefan-Baltzmann constant\n",
    "    \n",
    "    R_sw_arr = phys[:, :-1, 2] + (phys[:, 1:, 2] - phys[:, :-1, 2]) / 2\n",
    "    R_lw_arr = phys[:, :-1, 3] + (phys[:, 1:, 3] - phys[:, :-1, 3]) / 2\n",
    "    R_lw_out_arr = e_s * sigma * (torch.pow(surf_temps[:] + 273.15, 4))\n",
    "    R_lw_out_arr = R_lw_out_arr[:,:-1] + (R_lw_out_arr[:, 1:] - R_lw_out_arr[:, :-1]) / 2\n",
    "    \n",
    "    air_temp = phys[:, :-1, 4] \n",
    "    air_temp2 = phys[:, 1:, 4]\n",
    "    rel_hum = phys[:, :-1,5]\n",
    "    rel_hum2 = phys[:, 1:,5]\n",
    "    ws = phys[:, :-1, 6]\n",
    "    ws2 = phys[:, 1:,6]\n",
    "    t_s = surf_temps[:, :-1]\n",
    "    t_s2 = surf_temps[:, 1:]\n",
    "    \n",
    "    E = calculate_heat_flux_latent(t_s, air_temp, rel_hum, ws)\n",
    "    H = calculate_heat_flux_sensible(t_s, air_temp, rel_hum, ws)\n",
    "    E2 = calculate_heat_flux_latent(t_s2, air_temp2, rel_hum2, ws2)\n",
    "    H2 = calculate_heat_flux_sensible(t_s2, air_temp2, rel_hum2, ws2)\n",
    "    E = (E + E2) / 2\n",
    "    H = (H + H2) / 2\n",
    "    fluxes = (R_sw_arr[:,:-1] * (1-alpha_sw) + \\\n",
    "              R_lw_arr[:,:-1] * (1-alpha_lw) - \\\n",
    "              R_lw_out_arr[:,:-1] + E[:,:-1] + H[:, :-1])\n",
    "    return fluxes\n",
    "\n",
    "\n",
    "def EC_loss(preds, phys, depth_areas, n_depths, ec_threshold):\n",
    "    \n",
    "    densities = density_calculation(preds)\n",
    "    lake_energies = lake_energy_calculation(preds, densities, depth_areas=depth_areas)\n",
    "    lake_energy_deltas = calculate_lake_energy_deltas(lake_energies, None, depth_areas[0])\n",
    "\n",
    "    lake_energy_deltas = lake_energy_deltas[:,1:]\n",
    "\n",
    "    surf_phys = phys[:, 0, :, :]\n",
    "    surf_pred = preds[:, 0, :]\n",
    "\n",
    "    lake_energy_fluxes = energy_fluxes_calculation(surf_phys, surf_pred)\n",
    "    diff_vec = torch.abs(lake_energy_deltas - lake_energy_fluxes) \n",
    "\n",
    "    tmp_mask = 1 - phys[:, 0, 1:-1, 9] \n",
    "    tmp_loss = torch.mean(diff_vec * tmp_mask, axis = 1)\n",
    "\n",
    "\n",
    "    ec_threshold = 20\n",
    "    diff_per_set = torch.clamp(tmp_loss - ec_threshold, min=0, max=999999)\n",
    "    diff_loss = torch.mean(diff_per_set)\n",
    "    return diff_loss\n",
    "\n",
    "\n",
    "\n",
    "class LakeLoss:\n",
    "    def __init__(self,\n",
    "                 elam = 0.005, ## loss weight\n",
    "                 n_depths = None, \n",
    "                 depth_areas = None, \n",
    "                 ec_threshold = None, \n",
    "                 depth_loss = False, \n",
    "                 ec_loss = False):\n",
    "        self.n_depths = n_depths\n",
    "        self.depth_loss = depth_loss\n",
    "        self.ec_loss =ec_loss\n",
    "        self.depth_areas = depth_areas\n",
    "        self.ec_threshold = ec_threshold\n",
    "        self.elam = elam\n",
    "        \n",
    "        if self.ec_loss:\n",
    "            if depth_areas is None:\n",
    "                self.depth_areas = torch.Tensor([\n",
    "                39865825,38308175,38308175,35178625,35178625,33403850,31530150,31530150,30154150,30154150,29022000,\n",
    "                29022000,28063625,28063625,27501875,26744500,26744500,26084050,26084050,25310550,24685650,24685650,\n",
    "                23789125,23789125,22829450,22829450,21563875,21563875,20081675,18989925,18989925,17240525,17240525,\n",
    "                15659325,14100275,14100275,12271400,12271400,9962525,9962525,7777250,7777250,5956775,4039800,4039800,\n",
    "                2560125,2560125,820925,820925,216125])\n",
    "                self.n_depths = 50\n",
    "                self.ec_threshold = 24\n",
    "    \n",
    "    def __call__(self, pred, label, mask, phy):\n",
    "        \n",
    "        rmse_loss_val = self.weighted_rmse_loss(pred, label, mask)\n",
    "        if self.ec_loss:\n",
    "            ec_loss_val = EC_loss(pred, phy, self.depth_areas, self.n_depths, self.ec_threshold)\n",
    "            total_loss = rmse_loss_val + self.elam * ec_loss_val\n",
    "        else:\n",
    "            total_loss = rmse_loss_val\n",
    "            \n",
    "        return total_loss\n",
    "        \n",
    "    \n",
    "    def weighted_rmse_loss(self, input, target, weight):\n",
    "        # defined weighted rmse loss\n",
    "        # used in model training\n",
    "        # weight means mask\n",
    "        return torch.sqrt(torch.sum(weight * (input - target) ** 2) / torch.sum(weight))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "import logging\n",
    "import time\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def get_module_forward_input_names(module: nn.Module):\n",
    "    params = inspect.signature(module.forward).parameters\n",
    "    param_names = [k for k, v in params.items() if not str(v).startswith(\"*\")]\n",
    "    return param_names\n",
    "\n",
    "logging.basicConfig(level = 'INFO', # DEBUG\n",
    "        format = \"%(asctime)s %(levelname)s:%(lineno)d] %(message)s\",\n",
    "        datefmt = \"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "class Trainer:\n",
    "    def __init__(self, \n",
    "                 epochs = 500,\n",
    "                 learning_rate = 1e-2,\n",
    "                 device = 'cpu',\n",
    "                 early_stopping_patience = 3):\n",
    "        \n",
    "        self.epochs = epochs\n",
    "        self.learning_rate = learning_rate\n",
    "        self.device = device\n",
    "        self.early_stopping_patience = early_stopping_patience\n",
    "    \n",
    "    def __call__(self, \n",
    "                 model,\n",
    "                 loss_func,\n",
    "                 train_loader, \n",
    "                 valid_loader):\n",
    "        is_validation_available = valid_loader is not None\n",
    "        model.to(self.device)\n",
    "        self.input_names = get_module_forward_input_names(model)\n",
    "        optimizer = optim.Adam(model.parameters(), lr=self.learning_rate)\n",
    "        \n",
    "        patience = 0\n",
    "        \n",
    "        epoch_info = {\n",
    "                \"epoch_no\": -1,\n",
    "                \"loss\": np.Inf,}\n",
    "        \n",
    "        for epoch_no in range(self.epochs):\n",
    "\n",
    "            epoch_loss = self.loop(epoch_no, model, loss_func, train_loader, optimizer, is_train = True)\n",
    "            if is_validation_available:\n",
    "                epoch_loss = self.loop(\n",
    "                    epoch_no, model, loss_func, valid_loader, optimizer, is_train=False\n",
    "                    )\n",
    "            if epoch_loss < epoch_info['loss']:\n",
    "                epoch_info['loss'] = epoch_loss\n",
    "                epoch_info['epoch_no'] = epoch_no\n",
    "                patience = 0\n",
    "            else:\n",
    "                patience += 1\n",
    "            if patience >= self.early_stopping_patience:\n",
    "                logging.info(\"Early Stopping.\")\n",
    "                break\n",
    "        \n",
    "        self.epoch_info = epoch_info\n",
    "        return model, epoch_info\n",
    "        \n",
    "        \n",
    "    \n",
    "    def loop(self, epoch_no, model, loss_func, batch_iter, optimizer, is_train):\n",
    "        epoch_loss = 0\n",
    "        tic = time.time()\n",
    "        with tqdm(batch_iter, disable = not is_train) as it:\n",
    "            for batch_no, data_entry in enumerate(it, start = 1):\n",
    "                optimizer.zero_grad()\n",
    "                inputs = [data_entry[k].to(self.device) for k in self.input_names]\n",
    "                if is_train:\n",
    "                    output = model(*inputs)\n",
    "                else:\n",
    "                    with torch.no_grad():\n",
    "                        output = model(*inputs)\n",
    "\n",
    "                loss = loss_func(output, data_entry['label'], data_entry['mask'], data_entry['phy'])\n",
    "                if is_train:\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                epoch_loss += loss.item()\n",
    "                \n",
    "            lv = epoch_loss/batch_no\n",
    "            it.set_postfix(\n",
    "                    ordered_dict={\n",
    "                        \"epoch\": f\"{epoch_no + 1}/{self.epochs}\",\n",
    "                        (\"\" if is_train else \"validation_\")\n",
    "                        + \"avg_epoch_loss\": lv,},refresh=False,)\n",
    "            \n",
    "        toc = time.time()  \n",
    "        \n",
    "        if is_train:\n",
    "            logging.info(\"Epoch[%d] Elapsed time %.3f seconds\",\n",
    "                epoch_no,\n",
    "                (toc - tic),)\n",
    "        logging.info(\"Epoch[%d] Evaluation metric '%s'=%.4f\",\n",
    "                epoch_no,\n",
    "                (\"\" if is_train else \"validation_\") + \"epoch_loss\",\n",
    "                lv, )\n",
    "        return lv\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "\n",
    "class LSTM_base(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, hidden_dim, n_layers):\n",
    "        super(LSTM_base, self).__init__()\n",
    "        self.output_size = output_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.lstm = nn.LSTM(input_size = input_dim, \n",
    "                            hidden_size = hidden_dim, \n",
    "                            num_layers = n_layers, \n",
    "                            batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        batch, depth, seq_len, feat_dim = x.shape\n",
    "        \n",
    "        x = x.reshape(batch * depth, seq_len, feat_dim)\n",
    "        lstm_out, hidden = self.lstm(x)\n",
    "        out = self.fc(lstm_out)\n",
    "        out = out.reshape(batch, depth, seq_len, 1).squeeze()\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import inspect\n",
    "from  tqdm import tqdm\n",
    "\n",
    "def get_module_forward_input_names(module: nn.Module):\n",
    "    params = inspect.signature(module.forward).parameters\n",
    "    param_names = [k for k, v in params.items() if not str(v).startswith(\"*\")]\n",
    "    return param_names\n",
    "\n",
    "\n",
    "def model_inference(model, test_loader, device = 'cpu'):\n",
    "    epoch_loss = 0\n",
    "    tic = time.time()\n",
    "    \n",
    "    input_names = get_module_forward_input_names(model)\n",
    "    with tqdm(test_loader, disable = True) as it:\n",
    "        for batch_no, data_entry in enumerate(it, start = 1):\n",
    "            inputs = [data_entry[k].to(device) for k in input_names]\n",
    "            with torch.no_grad():\n",
    "                output = model(*inputs)\n",
    "            target = data_entry['label']\n",
    "            weight = data_entry['mask']\n",
    "            loss = torch.sqrt(torch.sum(weight * (output - target) ** 2) / torch.sum(weight))\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        lv = epoch_loss/len(te_loader)\n",
    "\n",
    "    toc = time.time()\n",
    "    return lv, toc - tic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-Trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [00:01<00:00,  8.69it/s]\n",
      "2022-03-01 17:37:30 INFO:101] Epoch[0] Elapsed time 1.391 seconds\n",
      "2022-03-01 17:37:30 INFO:104] Epoch[0] Evaluation metric 'epoch_loss'=10.7066\n",
      "2022-03-01 17:37:30 INFO:104] Epoch[0] Evaluation metric 'validation_epoch_loss'=10.3925\n",
      "100%|██████████| 12/12 [00:01<00:00, 11.66it/s]\n",
      "2022-03-01 17:37:31 INFO:101] Epoch[1] Elapsed time 1.031 seconds\n",
      "2022-03-01 17:37:31 INFO:104] Epoch[1] Evaluation metric 'epoch_loss'=10.0185\n",
      "2022-03-01 17:37:31 INFO:104] Epoch[1] Evaluation metric 'validation_epoch_loss'=9.3409\n",
      "100%|██████████| 12/12 [00:01<00:00, 11.89it/s]\n",
      "2022-03-01 17:37:32 INFO:101] Epoch[2] Elapsed time 1.011 seconds\n",
      "2022-03-01 17:37:32 INFO:104] Epoch[2] Evaluation metric 'epoch_loss'=8.7889\n",
      "2022-03-01 17:37:33 INFO:104] Epoch[2] Evaluation metric 'validation_epoch_loss'=8.1502\n",
      "100%|██████████| 12/12 [00:01<00:00, 11.87it/s]\n",
      "2022-03-01 17:37:34 INFO:101] Epoch[3] Elapsed time 1.012 seconds\n",
      "2022-03-01 17:37:34 INFO:104] Epoch[3] Evaluation metric 'epoch_loss'=7.7786\n",
      "2022-03-01 17:37:34 INFO:104] Epoch[3] Evaluation metric 'validation_epoch_loss'=7.2948\n",
      "100%|██████████| 12/12 [00:01<00:00, 11.93it/s]\n",
      "2022-03-01 17:37:35 INFO:101] Epoch[4] Elapsed time 1.007 seconds\n",
      "2022-03-01 17:37:35 INFO:104] Epoch[4] Evaluation metric 'epoch_loss'=7.0046\n",
      "2022-03-01 17:37:35 INFO:104] Epoch[4] Evaluation metric 'validation_epoch_loss'=6.5910\n",
      "100%|██████████| 12/12 [00:01<00:00, 11.87it/s]\n",
      "2022-03-01 17:37:36 INFO:101] Epoch[5] Elapsed time 1.012 seconds\n",
      "2022-03-01 17:37:36 INFO:104] Epoch[5] Evaluation metric 'epoch_loss'=6.3146\n",
      "2022-03-01 17:37:37 INFO:104] Epoch[5] Evaluation metric 'validation_epoch_loss'=5.9247\n",
      "100%|██████████| 12/12 [00:00<00:00, 12.04it/s]\n",
      "2022-03-01 17:37:38 INFO:101] Epoch[6] Elapsed time 0.998 seconds\n",
      "2022-03-01 17:37:38 INFO:104] Epoch[6] Evaluation metric 'epoch_loss'=5.6688\n",
      "2022-03-01 17:37:38 INFO:104] Epoch[6] Evaluation metric 'validation_epoch_loss'=5.3409\n",
      "100%|██████████| 12/12 [00:01<00:00, 11.92it/s]\n",
      "2022-03-01 17:37:39 INFO:101] Epoch[7] Elapsed time 1.008 seconds\n",
      "2022-03-01 17:37:39 INFO:104] Epoch[7] Evaluation metric 'epoch_loss'=5.1463\n",
      "2022-03-01 17:37:39 INFO:104] Epoch[7] Evaluation metric 'validation_epoch_loss'=4.8478\n",
      "100%|██████████| 12/12 [00:00<00:00, 12.07it/s]\n",
      "2022-03-01 17:37:40 INFO:101] Epoch[8] Elapsed time 0.996 seconds\n",
      "2022-03-01 17:37:40 INFO:104] Epoch[8] Evaluation metric 'epoch_loss'=4.6346\n",
      "2022-03-01 17:37:41 INFO:104] Epoch[8] Evaluation metric 'validation_epoch_loss'=4.4245\n",
      "100%|██████████| 12/12 [00:00<00:00, 12.03it/s]\n",
      "2022-03-01 17:37:42 INFO:101] Epoch[9] Elapsed time 0.999 seconds\n",
      "2022-03-01 17:37:42 INFO:104] Epoch[9] Evaluation metric 'epoch_loss'=4.2548\n",
      "2022-03-01 17:37:42 INFO:104] Epoch[9] Evaluation metric 'validation_epoch_loss'=4.0906\n",
      "100%|██████████| 12/12 [00:00<00:00, 12.03it/s]\n",
      "2022-03-01 17:37:43 INFO:101] Epoch[10] Elapsed time 0.999 seconds\n",
      "2022-03-01 17:37:43 INFO:104] Epoch[10] Evaluation metric 'epoch_loss'=3.9619\n",
      "2022-03-01 17:37:43 INFO:104] Epoch[10] Evaluation metric 'validation_epoch_loss'=3.8374\n",
      "100%|██████████| 12/12 [00:00<00:00, 12.07it/s]\n",
      "2022-03-01 17:37:44 INFO:101] Epoch[11] Elapsed time 0.996 seconds\n",
      "2022-03-01 17:37:44 INFO:104] Epoch[11] Evaluation metric 'epoch_loss'=3.7405\n",
      "2022-03-01 17:37:45 INFO:104] Epoch[11] Evaluation metric 'validation_epoch_loss'=3.6463\n",
      "100%|██████████| 12/12 [00:00<00:00, 12.03it/s]\n",
      "2022-03-01 17:37:46 INFO:101] Epoch[12] Elapsed time 0.999 seconds\n",
      "2022-03-01 17:37:46 INFO:104] Epoch[12] Evaluation metric 'epoch_loss'=3.5290\n",
      "2022-03-01 17:37:46 INFO:104] Epoch[12] Evaluation metric 'validation_epoch_loss'=3.4606\n",
      "100%|██████████| 12/12 [00:00<00:00, 12.02it/s]\n",
      "2022-03-01 17:37:47 INFO:101] Epoch[13] Elapsed time 1.000 seconds\n",
      "2022-03-01 17:37:47 INFO:104] Epoch[13] Evaluation metric 'epoch_loss'=3.3368\n",
      "2022-03-01 17:37:47 INFO:104] Epoch[13] Evaluation metric 'validation_epoch_loss'=3.3078\n",
      "100%|██████████| 12/12 [00:01<00:00, 11.93it/s]\n",
      "2022-03-01 17:37:48 INFO:101] Epoch[14] Elapsed time 1.007 seconds\n",
      "2022-03-01 17:37:48 INFO:104] Epoch[14] Evaluation metric 'epoch_loss'=3.2074\n",
      "2022-03-01 17:37:49 INFO:104] Epoch[14] Evaluation metric 'validation_epoch_loss'=3.1750\n",
      "100%|██████████| 12/12 [00:00<00:00, 12.03it/s]\n",
      "2022-03-01 17:37:50 INFO:101] Epoch[15] Elapsed time 0.999 seconds\n",
      "2022-03-01 17:37:50 INFO:104] Epoch[15] Evaluation metric 'epoch_loss'=3.0545\n",
      "2022-03-01 17:37:50 INFO:104] Epoch[15] Evaluation metric 'validation_epoch_loss'=3.0224\n",
      "100%|██████████| 12/12 [00:00<00:00, 12.04it/s]\n",
      "2022-03-01 17:37:51 INFO:101] Epoch[16] Elapsed time 0.998 seconds\n",
      "2022-03-01 17:37:51 INFO:104] Epoch[16] Evaluation metric 'epoch_loss'=2.9183\n",
      "2022-03-01 17:37:51 INFO:104] Epoch[16] Evaluation metric 'validation_epoch_loss'=2.8724\n",
      "100%|██████████| 12/12 [00:01<00:00, 11.98it/s]\n",
      "2022-03-01 17:37:52 INFO:101] Epoch[17] Elapsed time 1.003 seconds\n",
      "2022-03-01 17:37:52 INFO:104] Epoch[17] Evaluation metric 'epoch_loss'=2.7449\n",
      "2022-03-01 17:37:53 INFO:104] Epoch[17] Evaluation metric 'validation_epoch_loss'=2.7383\n",
      "100%|██████████| 12/12 [00:00<00:00, 12.07it/s]\n",
      "2022-03-01 17:37:54 INFO:101] Epoch[18] Elapsed time 0.995 seconds\n",
      "2022-03-01 17:37:54 INFO:104] Epoch[18] Evaluation metric 'epoch_loss'=2.6157\n",
      "2022-03-01 17:37:54 INFO:104] Epoch[18] Evaluation metric 'validation_epoch_loss'=2.6127\n",
      "100%|██████████| 12/12 [00:00<00:00, 12.00it/s]\n",
      "2022-03-01 17:37:55 INFO:101] Epoch[19] Elapsed time 1.001 seconds\n",
      "2022-03-01 17:37:55 INFO:104] Epoch[19] Evaluation metric 'epoch_loss'=2.4551\n",
      "2022-03-01 17:37:55 INFO:104] Epoch[19] Evaluation metric 'validation_epoch_loss'=2.4709\n",
      "100%|██████████| 12/12 [00:00<00:00, 12.01it/s]\n",
      "2022-03-01 17:37:56 INFO:101] Epoch[20] Elapsed time 1.001 seconds\n",
      "2022-03-01 17:37:56 INFO:104] Epoch[20] Evaluation metric 'epoch_loss'=2.3488\n",
      "2022-03-01 17:37:57 INFO:104] Epoch[20] Evaluation metric 'validation_epoch_loss'=2.3530\n",
      "100%|██████████| 12/12 [00:00<00:00, 12.10it/s]\n",
      "2022-03-01 17:37:58 INFO:101] Epoch[21] Elapsed time 0.993 seconds\n",
      "2022-03-01 17:37:58 INFO:104] Epoch[21] Evaluation metric 'epoch_loss'=2.1967\n",
      "2022-03-01 17:37:58 INFO:104] Epoch[21] Evaluation metric 'validation_epoch_loss'=2.2387\n",
      "100%|██████████| 12/12 [00:00<00:00, 12.08it/s]\n",
      "2022-03-01 17:37:59 INFO:101] Epoch[22] Elapsed time 0.995 seconds\n",
      "2022-03-01 17:37:59 INFO:104] Epoch[22] Evaluation metric 'epoch_loss'=2.0771\n",
      "2022-03-01 17:37:59 INFO:104] Epoch[22] Evaluation metric 'validation_epoch_loss'=2.1277\n",
      "100%|██████████| 12/12 [00:00<00:00, 12.04it/s]\n",
      "2022-03-01 17:38:00 INFO:101] Epoch[23] Elapsed time 0.998 seconds\n",
      "2022-03-01 17:38:00 INFO:104] Epoch[23] Evaluation metric 'epoch_loss'=1.9531\n",
      "2022-03-01 17:38:01 INFO:104] Epoch[23] Evaluation metric 'validation_epoch_loss'=2.0120\n",
      "100%|██████████| 12/12 [00:01<00:00, 11.96it/s]\n",
      "2022-03-01 17:38:02 INFO:101] Epoch[24] Elapsed time 1.005 seconds\n",
      "2022-03-01 17:38:02 INFO:104] Epoch[24] Evaluation metric 'epoch_loss'=1.8336\n",
      "2022-03-01 17:38:02 INFO:104] Epoch[24] Evaluation metric 'validation_epoch_loss'=1.9269\n",
      "100%|██████████| 12/12 [00:00<00:00, 12.09it/s]\n",
      "2022-03-01 17:38:03 INFO:101] Epoch[25] Elapsed time 0.994 seconds\n",
      "2022-03-01 17:38:03 INFO:104] Epoch[25] Evaluation metric 'epoch_loss'=1.7311\n",
      "2022-03-01 17:38:03 INFO:104] Epoch[25] Evaluation metric 'validation_epoch_loss'=1.8639\n",
      "100%|██████████| 12/12 [00:00<00:00, 12.11it/s]\n",
      "2022-03-01 17:38:04 INFO:101] Epoch[26] Elapsed time 0.992 seconds\n",
      "2022-03-01 17:38:04 INFO:104] Epoch[26] Evaluation metric 'epoch_loss'=1.6627\n",
      "2022-03-01 17:38:05 INFO:104] Epoch[26] Evaluation metric 'validation_epoch_loss'=1.7779\n",
      "100%|██████████| 12/12 [00:00<00:00, 12.04it/s]\n",
      "2022-03-01 17:38:06 INFO:101] Epoch[27] Elapsed time 0.998 seconds\n",
      "2022-03-01 17:38:06 INFO:104] Epoch[27] Evaluation metric 'epoch_loss'=1.5540\n",
      "2022-03-01 17:38:06 INFO:104] Epoch[27] Evaluation metric 'validation_epoch_loss'=1.6962\n",
      "100%|██████████| 12/12 [00:01<00:00, 11.84it/s]\n",
      "2022-03-01 17:38:07 INFO:101] Epoch[28] Elapsed time 1.015 seconds\n",
      "2022-03-01 17:38:07 INFO:104] Epoch[28] Evaluation metric 'epoch_loss'=1.4691\n",
      "2022-03-01 17:38:07 INFO:104] Epoch[28] Evaluation metric 'validation_epoch_loss'=1.6942\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [00:01<00:00, 12.00it/s]\n",
      "2022-03-01 17:38:08 INFO:101] Epoch[29] Elapsed time 1.002 seconds\n",
      "2022-03-01 17:38:08 INFO:104] Epoch[29] Evaluation metric 'epoch_loss'=1.4280\n",
      "2022-03-01 17:38:09 INFO:104] Epoch[29] Evaluation metric 'validation_epoch_loss'=1.6073\n",
      "100%|██████████| 12/12 [00:00<00:00, 12.05it/s]\n",
      "2022-03-01 17:38:10 INFO:101] Epoch[30] Elapsed time 0.997 seconds\n",
      "2022-03-01 17:38:10 INFO:104] Epoch[30] Evaluation metric 'epoch_loss'=1.3682\n",
      "2022-03-01 17:38:10 INFO:104] Epoch[30] Evaluation metric 'validation_epoch_loss'=1.5415\n",
      "100%|██████████| 12/12 [00:00<00:00, 12.08it/s]\n",
      "2022-03-01 17:38:11 INFO:101] Epoch[31] Elapsed time 0.994 seconds\n",
      "2022-03-01 17:38:11 INFO:104] Epoch[31] Evaluation metric 'epoch_loss'=1.3171\n",
      "2022-03-01 17:38:11 INFO:104] Epoch[31] Evaluation metric 'validation_epoch_loss'=1.4994\n",
      "100%|██████████| 12/12 [00:00<00:00, 12.14it/s]\n",
      "2022-03-01 17:38:12 INFO:101] Epoch[32] Elapsed time 0.990 seconds\n",
      "2022-03-01 17:38:12 INFO:104] Epoch[32] Evaluation metric 'epoch_loss'=1.2681\n",
      "2022-03-01 17:38:13 INFO:104] Epoch[32] Evaluation metric 'validation_epoch_loss'=1.4879\n",
      "100%|██████████| 12/12 [00:00<00:00, 12.15it/s]\n",
      "2022-03-01 17:38:14 INFO:101] Epoch[33] Elapsed time 0.989 seconds\n",
      "2022-03-01 17:38:14 INFO:104] Epoch[33] Evaluation metric 'epoch_loss'=1.2153\n",
      "2022-03-01 17:38:14 INFO:104] Epoch[33] Evaluation metric 'validation_epoch_loss'=1.4940\n",
      "100%|██████████| 12/12 [00:00<00:00, 12.12it/s]\n",
      "2022-03-01 17:38:15 INFO:101] Epoch[34] Elapsed time 0.992 seconds\n",
      "2022-03-01 17:38:15 INFO:104] Epoch[34] Evaluation metric 'epoch_loss'=1.1827\n",
      "2022-03-01 17:38:15 INFO:104] Epoch[34] Evaluation metric 'validation_epoch_loss'=1.4406\n",
      "100%|██████████| 12/12 [00:00<00:00, 12.12it/s]\n",
      "2022-03-01 17:38:16 INFO:101] Epoch[35] Elapsed time 0.992 seconds\n",
      "2022-03-01 17:38:16 INFO:104] Epoch[35] Evaluation metric 'epoch_loss'=1.2010\n",
      "2022-03-01 17:38:17 INFO:104] Epoch[35] Evaluation metric 'validation_epoch_loss'=1.3875\n",
      "100%|██████████| 12/12 [00:00<00:00, 12.13it/s]\n",
      "2022-03-01 17:38:18 INFO:101] Epoch[36] Elapsed time 0.991 seconds\n",
      "2022-03-01 17:38:18 INFO:104] Epoch[36] Evaluation metric 'epoch_loss'=1.1240\n",
      "2022-03-01 17:38:18 INFO:104] Epoch[36] Evaluation metric 'validation_epoch_loss'=1.4174\n",
      "100%|██████████| 12/12 [00:00<00:00, 12.19it/s]\n",
      "2022-03-01 17:38:19 INFO:101] Epoch[37] Elapsed time 0.986 seconds\n",
      "2022-03-01 17:38:19 INFO:104] Epoch[37] Evaluation metric 'epoch_loss'=1.1029\n",
      "2022-03-01 17:38:19 INFO:104] Epoch[37] Evaluation metric 'validation_epoch_loss'=1.3543\n",
      "100%|██████████| 12/12 [00:01<00:00, 11.84it/s]\n",
      "2022-03-01 17:38:20 INFO:101] Epoch[38] Elapsed time 1.015 seconds\n",
      "2022-03-01 17:38:20 INFO:104] Epoch[38] Evaluation metric 'epoch_loss'=1.0877\n",
      "2022-03-01 17:38:21 INFO:104] Epoch[38] Evaluation metric 'validation_epoch_loss'=1.3504\n",
      "100%|██████████| 12/12 [00:00<00:00, 12.08it/s]\n",
      "2022-03-01 17:38:22 INFO:101] Epoch[39] Elapsed time 0.995 seconds\n",
      "2022-03-01 17:38:22 INFO:104] Epoch[39] Evaluation metric 'epoch_loss'=1.1042\n",
      "2022-03-01 17:38:22 INFO:104] Epoch[39] Evaluation metric 'validation_epoch_loss'=1.3356\n",
      "100%|██████████| 12/12 [00:00<00:00, 12.17it/s]\n",
      "2022-03-01 17:38:23 INFO:101] Epoch[40] Elapsed time 0.987 seconds\n",
      "2022-03-01 17:38:23 INFO:104] Epoch[40] Evaluation metric 'epoch_loss'=1.0459\n",
      "2022-03-01 17:38:23 INFO:104] Epoch[40] Evaluation metric 'validation_epoch_loss'=1.3092\n",
      "100%|██████████| 12/12 [00:00<00:00, 12.09it/s]\n",
      "2022-03-01 17:38:24 INFO:101] Epoch[41] Elapsed time 0.994 seconds\n",
      "2022-03-01 17:38:24 INFO:104] Epoch[41] Evaluation metric 'epoch_loss'=1.0127\n",
      "2022-03-01 17:38:25 INFO:104] Epoch[41] Evaluation metric 'validation_epoch_loss'=1.3023\n",
      "100%|██████████| 12/12 [00:00<00:00, 12.14it/s]\n",
      "2022-03-01 17:38:26 INFO:101] Epoch[42] Elapsed time 0.990 seconds\n",
      "2022-03-01 17:38:26 INFO:104] Epoch[42] Evaluation metric 'epoch_loss'=0.9888\n",
      "2022-03-01 17:38:26 INFO:104] Epoch[42] Evaluation metric 'validation_epoch_loss'=1.3211\n",
      "100%|██████████| 12/12 [00:01<00:00, 12.00it/s]\n",
      "2022-03-01 17:38:27 INFO:101] Epoch[43] Elapsed time 1.002 seconds\n",
      "2022-03-01 17:38:27 INFO:104] Epoch[43] Evaluation metric 'epoch_loss'=1.0464\n",
      "2022-03-01 17:38:27 INFO:104] Epoch[43] Evaluation metric 'validation_epoch_loss'=1.3121\n",
      "100%|██████████| 12/12 [00:00<00:00, 12.11it/s]\n",
      "2022-03-01 17:38:28 INFO:101] Epoch[44] Elapsed time 0.992 seconds\n",
      "2022-03-01 17:38:28 INFO:104] Epoch[44] Evaluation metric 'epoch_loss'=1.0030\n",
      "2022-03-01 17:38:28 INFO:104] Epoch[44] Evaluation metric 'validation_epoch_loss'=1.2993\n",
      "100%|██████████| 12/12 [00:00<00:00, 12.13it/s]\n",
      "2022-03-01 17:38:29 INFO:101] Epoch[45] Elapsed time 0.991 seconds\n",
      "2022-03-01 17:38:29 INFO:104] Epoch[45] Evaluation metric 'epoch_loss'=1.0162\n",
      "2022-03-01 17:38:30 INFO:104] Epoch[45] Evaluation metric 'validation_epoch_loss'=1.2962\n",
      "100%|██████████| 12/12 [00:00<00:00, 12.12it/s]\n",
      "2022-03-01 17:38:31 INFO:101] Epoch[46] Elapsed time 0.991 seconds\n",
      "2022-03-01 17:38:31 INFO:104] Epoch[46] Evaluation metric 'epoch_loss'=0.9786\n",
      "2022-03-01 17:38:31 INFO:104] Epoch[46] Evaluation metric 'validation_epoch_loss'=1.2828\n",
      "100%|██████████| 12/12 [00:00<00:00, 12.13it/s]\n",
      "2022-03-01 17:38:32 INFO:101] Epoch[47] Elapsed time 0.991 seconds\n",
      "2022-03-01 17:38:32 INFO:104] Epoch[47] Evaluation metric 'epoch_loss'=0.9553\n",
      "2022-03-01 17:38:32 INFO:104] Epoch[47] Evaluation metric 'validation_epoch_loss'=1.2731\n",
      "100%|██████████| 12/12 [00:00<00:00, 12.10it/s]\n",
      "2022-03-01 17:38:33 INFO:101] Epoch[48] Elapsed time 0.993 seconds\n",
      "2022-03-01 17:38:33 INFO:104] Epoch[48] Evaluation metric 'epoch_loss'=0.9753\n",
      "2022-03-01 17:38:34 INFO:104] Epoch[48] Evaluation metric 'validation_epoch_loss'=1.3010\n",
      "100%|██████████| 12/12 [00:00<00:00, 12.07it/s]\n",
      "2022-03-01 17:38:35 INFO:101] Epoch[49] Elapsed time 0.996 seconds\n",
      "2022-03-01 17:38:35 INFO:104] Epoch[49] Evaluation metric 'epoch_loss'=0.9900\n",
      "2022-03-01 17:38:35 INFO:104] Epoch[49] Evaluation metric 'validation_epoch_loss'=1.2739\n",
      "100%|██████████| 12/12 [00:00<00:00, 12.12it/s]\n",
      "2022-03-01 17:38:36 INFO:101] Epoch[50] Elapsed time 0.992 seconds\n",
      "2022-03-01 17:38:36 INFO:104] Epoch[50] Evaluation metric 'epoch_loss'=0.9705\n",
      "2022-03-01 17:38:36 INFO:104] Epoch[50] Evaluation metric 'validation_epoch_loss'=1.3067\n",
      "2022-03-01 17:38:36 INFO:64] Early Stopping.\n"
     ]
    }
   ],
   "source": [
    "import os, random, warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "data_dir = '../Data/'\n",
    "output_dir = '../Output/'\n",
    "\n",
    "strides = 352//2\n",
    "window_size = 352\n",
    "\n",
    "tr_loader, va_loader, full_loader, te_loader = get_dataloader(path = data_dir, \n",
    "                                                              window_size = window_size,\n",
    "                                                              strides = strides,\n",
    "                                                              batch_size = 2,\n",
    "                                                              mask_rate = 0,\n",
    "                                                              simulate = True,\n",
    "                                                              )\n",
    "\n",
    "\n",
    "\n",
    "input_size = 9\n",
    "state_size = 7\n",
    "\n",
    "trainer = Trainer(device = 'cpu')\n",
    "\n",
    "model = LSTM_base(input_dim = input_size, output_dim = 1, hidden_dim = state_size, n_layers = 1)\n",
    "\n",
    "loss_func = LakeLoss(ec_loss = False)\n",
    "model, epoch_info = trainer(model, loss_func, tr_loader, va_loader)\n",
    "\n",
    "\n",
    "# trainer.epochs = epoch_info['epoch_no']\n",
    "# trainer.early_stopping_patience = 100\n",
    "# model, epoch_info_retrain = trainer(model, loss_func, full_loader, full_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test on rely data without training on real data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss is: 2.0938355227311454\n"
     ]
    }
   ],
   "source": [
    "tr_loader, va_loader, full_loader, te_loader = get_dataloader(path = data_dir, \n",
    "                                                              window_size = window_size,\n",
    "                                                              strides = strides,\n",
    "                                                              batch_size = 2,\n",
    "                                                              mask_rate = 0,\n",
    "                                                              simulate = False,\n",
    "                                                              )\n",
    "\n",
    "\n",
    "\n",
    "test_loss, test_time = model_inference(model, te_loader)\n",
    "print('Test loss is:', test_loss)\n",
    "\n",
    "# 1.6136637772481466"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test with fine-tuneon real data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [00:01<00:00, 11.68it/s]\n",
      "2022-02-28 14:17:09 INFO:101] Epoch[0] Elapsed time 1.029 seconds\n",
      "2022-02-28 14:17:09 INFO:104] Epoch[0] Evaluation metric 'epoch_loss'=2.0869\n",
      "2022-02-28 14:17:09 INFO:104] Epoch[0] Evaluation metric 'validation_epoch_loss'=1.7556\n",
      "100%|██████████| 12/12 [00:01<00:00, 11.26it/s]\n",
      "2022-02-28 14:17:10 INFO:101] Epoch[1] Elapsed time 1.068 seconds\n",
      "2022-02-28 14:17:10 INFO:104] Epoch[1] Evaluation metric 'epoch_loss'=1.6813\n",
      "2022-02-28 14:17:11 INFO:104] Epoch[1] Evaluation metric 'validation_epoch_loss'=1.5679\n",
      "100%|██████████| 12/12 [00:01<00:00, 11.49it/s]\n",
      "2022-02-28 14:17:12 INFO:101] Epoch[2] Elapsed time 1.046 seconds\n",
      "2022-02-28 14:17:12 INFO:104] Epoch[2] Evaluation metric 'epoch_loss'=1.5755\n",
      "2022-02-28 14:17:12 INFO:104] Epoch[2] Evaluation metric 'validation_epoch_loss'=1.5094\n",
      "100%|██████████| 12/12 [00:01<00:00, 11.74it/s]\n",
      "2022-02-28 14:17:13 INFO:101] Epoch[3] Elapsed time 1.023 seconds\n",
      "2022-02-28 14:17:13 INFO:104] Epoch[3] Evaluation metric 'epoch_loss'=1.5368\n",
      "2022-02-28 14:17:13 INFO:104] Epoch[3] Evaluation metric 'validation_epoch_loss'=1.4699\n",
      "100%|██████████| 12/12 [00:01<00:00, 11.82it/s]\n",
      "2022-02-28 14:17:14 INFO:101] Epoch[4] Elapsed time 1.017 seconds\n",
      "2022-02-28 14:17:14 INFO:104] Epoch[4] Evaluation metric 'epoch_loss'=1.4957\n",
      "2022-02-28 14:17:15 INFO:104] Epoch[4] Evaluation metric 'validation_epoch_loss'=1.4483\n",
      "100%|██████████| 12/12 [00:01<00:00, 11.62it/s]\n",
      "2022-02-28 14:17:16 INFO:101] Epoch[5] Elapsed time 1.034 seconds\n",
      "2022-02-28 14:17:16 INFO:104] Epoch[5] Evaluation metric 'epoch_loss'=1.4824\n",
      "2022-02-28 14:17:16 INFO:104] Epoch[5] Evaluation metric 'validation_epoch_loss'=1.4261\n",
      "100%|██████████| 12/12 [00:01<00:00, 11.56it/s]\n",
      "2022-02-28 14:17:17 INFO:101] Epoch[6] Elapsed time 1.039 seconds\n",
      "2022-02-28 14:17:17 INFO:104] Epoch[6] Evaluation metric 'epoch_loss'=1.4587\n",
      "2022-02-28 14:17:18 INFO:104] Epoch[6] Evaluation metric 'validation_epoch_loss'=1.4083\n",
      "100%|██████████| 12/12 [00:01<00:00, 11.39it/s]\n",
      "2022-02-28 14:17:19 INFO:101] Epoch[7] Elapsed time 1.055 seconds\n",
      "2022-02-28 14:17:19 INFO:104] Epoch[7] Evaluation metric 'epoch_loss'=1.4446\n",
      "2022-02-28 14:17:19 INFO:104] Epoch[7] Evaluation metric 'validation_epoch_loss'=1.4022\n",
      "100%|██████████| 12/12 [00:01<00:00, 11.55it/s]\n",
      "2022-02-28 14:17:20 INFO:101] Epoch[8] Elapsed time 1.040 seconds\n",
      "2022-02-28 14:17:20 INFO:104] Epoch[8] Evaluation metric 'epoch_loss'=1.4389\n",
      "2022-02-28 14:17:20 INFO:104] Epoch[8] Evaluation metric 'validation_epoch_loss'=1.3891\n",
      "100%|██████████| 12/12 [00:01<00:00, 11.83it/s]\n",
      "2022-02-28 14:17:21 INFO:101] Epoch[9] Elapsed time 1.016 seconds\n",
      "2022-02-28 14:17:21 INFO:104] Epoch[9] Evaluation metric 'epoch_loss'=1.4256\n",
      "2022-02-28 14:17:22 INFO:104] Epoch[9] Evaluation metric 'validation_epoch_loss'=1.3791\n",
      "100%|██████████| 12/12 [00:01<00:00, 11.35it/s]\n",
      "2022-02-28 14:17:23 INFO:101] Epoch[10] Elapsed time 1.058 seconds\n",
      "2022-02-28 14:17:23 INFO:104] Epoch[10] Evaluation metric 'epoch_loss'=1.4200\n",
      "2022-02-28 14:17:23 INFO:104] Epoch[10] Evaluation metric 'validation_epoch_loss'=1.3684\n",
      "100%|██████████| 12/12 [00:01<00:00, 11.18it/s]\n",
      "2022-02-28 14:17:24 INFO:101] Epoch[11] Elapsed time 1.075 seconds\n",
      "2022-02-28 14:17:24 INFO:104] Epoch[11] Evaluation metric 'epoch_loss'=1.4056\n",
      "2022-02-28 14:17:24 INFO:104] Epoch[11] Evaluation metric 'validation_epoch_loss'=1.3629\n",
      "100%|██████████| 12/12 [00:01<00:00, 11.53it/s]\n",
      "2022-02-28 14:17:26 INFO:101] Epoch[12] Elapsed time 1.042 seconds\n",
      "2022-02-28 14:17:26 INFO:104] Epoch[12] Evaluation metric 'epoch_loss'=1.4067\n",
      "2022-02-28 14:17:26 INFO:104] Epoch[12] Evaluation metric 'validation_epoch_loss'=1.3578\n",
      "100%|██████████| 12/12 [00:01<00:00, 11.63it/s]\n",
      "2022-02-28 14:17:27 INFO:101] Epoch[13] Elapsed time 1.033 seconds\n",
      "2022-02-28 14:17:27 INFO:104] Epoch[13] Evaluation metric 'epoch_loss'=1.3964\n",
      "2022-02-28 14:17:27 INFO:104] Epoch[13] Evaluation metric 'validation_epoch_loss'=1.3899\n",
      "100%|██████████| 12/12 [00:01<00:00, 11.58it/s]\n",
      "2022-02-28 14:17:28 INFO:101] Epoch[14] Elapsed time 1.038 seconds\n",
      "2022-02-28 14:17:28 INFO:104] Epoch[14] Evaluation metric 'epoch_loss'=1.4291\n",
      "2022-02-28 14:17:29 INFO:104] Epoch[14] Evaluation metric 'validation_epoch_loss'=1.3733\n",
      "100%|██████████| 12/12 [00:01<00:00, 11.59it/s]\n",
      "2022-02-28 14:17:30 INFO:101] Epoch[15] Elapsed time 1.036 seconds\n",
      "2022-02-28 14:17:30 INFO:104] Epoch[15] Evaluation metric 'epoch_loss'=1.4086\n",
      "2022-02-28 14:17:30 INFO:104] Epoch[15] Evaluation metric 'validation_epoch_loss'=1.3693\n",
      "100%|██████████| 12/12 [00:01<00:00, 11.63it/s]\n",
      "2022-02-28 14:17:31 INFO:101] Epoch[16] Elapsed time 1.033 seconds\n",
      "2022-02-28 14:17:31 INFO:104] Epoch[16] Evaluation metric 'epoch_loss'=1.4102\n",
      "2022-02-28 14:17:31 INFO:104] Epoch[16] Evaluation metric 'validation_epoch_loss'=1.3786\n",
      "100%|██████████| 12/12 [00:01<00:00, 11.61it/s]\n",
      "2022-02-28 14:17:32 INFO:101] Epoch[17] Elapsed time 1.035 seconds\n",
      "2022-02-28 14:17:32 INFO:104] Epoch[17] Evaluation metric 'epoch_loss'=1.3974\n",
      "2022-02-28 14:17:33 INFO:104] Epoch[17] Evaluation metric 'validation_epoch_loss'=1.3417\n",
      "100%|██████████| 12/12 [00:01<00:00, 11.69it/s]\n",
      "2022-02-28 14:17:34 INFO:101] Epoch[18] Elapsed time 1.027 seconds\n",
      "2022-02-28 14:17:34 INFO:104] Epoch[18] Evaluation metric 'epoch_loss'=1.3833\n",
      "2022-02-28 14:17:34 INFO:104] Epoch[18] Evaluation metric 'validation_epoch_loss'=1.3278\n",
      "100%|██████████| 12/12 [00:01<00:00, 11.71it/s]\n",
      "2022-02-28 14:17:35 INFO:101] Epoch[19] Elapsed time 1.026 seconds\n",
      "2022-02-28 14:17:35 INFO:104] Epoch[19] Evaluation metric 'epoch_loss'=1.3671\n",
      "2022-02-28 14:17:35 INFO:104] Epoch[19] Evaluation metric 'validation_epoch_loss'=1.3423\n",
      "100%|██████████| 12/12 [00:01<00:00, 11.54it/s]\n",
      "2022-02-28 14:17:36 INFO:101] Epoch[20] Elapsed time 1.041 seconds\n",
      "2022-02-28 14:17:36 INFO:104] Epoch[20] Evaluation metric 'epoch_loss'=1.3753\n",
      "2022-02-28 14:17:37 INFO:104] Epoch[20] Evaluation metric 'validation_epoch_loss'=1.3572\n",
      "100%|██████████| 12/12 [00:01<00:00, 11.55it/s]\n",
      "2022-02-28 14:17:38 INFO:101] Epoch[21] Elapsed time 1.040 seconds\n",
      "2022-02-28 14:17:38 INFO:104] Epoch[21] Evaluation metric 'epoch_loss'=1.3727\n",
      "2022-02-28 14:17:38 INFO:104] Epoch[21] Evaluation metric 'validation_epoch_loss'=1.3169\n",
      "100%|██████████| 12/12 [00:01<00:00, 11.69it/s]\n",
      "2022-02-28 14:17:39 INFO:101] Epoch[22] Elapsed time 1.028 seconds\n",
      "2022-02-28 14:17:39 INFO:104] Epoch[22] Evaluation metric 'epoch_loss'=1.3526\n",
      "2022-02-28 14:17:40 INFO:104] Epoch[22] Evaluation metric 'validation_epoch_loss'=1.3037\n",
      "100%|██████████| 12/12 [00:01<00:00, 11.38it/s]\n",
      "2022-02-28 14:17:41 INFO:101] Epoch[23] Elapsed time 1.056 seconds\n",
      "2022-02-28 14:17:41 INFO:104] Epoch[23] Evaluation metric 'epoch_loss'=1.3405\n",
      "2022-02-28 14:17:41 INFO:104] Epoch[23] Evaluation metric 'validation_epoch_loss'=1.2956\n",
      "100%|██████████| 12/12 [00:01<00:00, 11.56it/s]\n",
      "2022-02-28 14:17:42 INFO:101] Epoch[24] Elapsed time 1.039 seconds\n",
      "2022-02-28 14:17:42 INFO:104] Epoch[24] Evaluation metric 'epoch_loss'=1.3372\n",
      "2022-02-28 14:17:42 INFO:104] Epoch[24] Evaluation metric 'validation_epoch_loss'=1.3145\n",
      "100%|██████████| 12/12 [00:01<00:00, 11.52it/s]\n",
      "2022-02-28 14:17:43 INFO:101] Epoch[25] Elapsed time 1.043 seconds\n",
      "2022-02-28 14:17:43 INFO:104] Epoch[25] Evaluation metric 'epoch_loss'=1.3422\n",
      "2022-02-28 14:17:44 INFO:104] Epoch[25] Evaluation metric 'validation_epoch_loss'=1.3246\n",
      "100%|██████████| 12/12 [00:01<00:00, 11.57it/s]\n",
      "2022-02-28 14:17:45 INFO:101] Epoch[26] Elapsed time 1.038 seconds\n",
      "2022-02-28 14:17:45 INFO:104] Epoch[26] Evaluation metric 'epoch_loss'=1.3384\n",
      "2022-02-28 14:17:45 INFO:104] Epoch[26] Evaluation metric 'validation_epoch_loss'=1.2962\n",
      "100%|██████████| 12/12 [00:01<00:00, 11.61it/s]\n",
      "2022-02-28 14:17:46 INFO:101] Epoch[27] Elapsed time 1.035 seconds\n",
      "2022-02-28 14:17:46 INFO:104] Epoch[27] Evaluation metric 'epoch_loss'=1.3239\n",
      "2022-02-28 14:17:46 INFO:104] Epoch[27] Evaluation metric 'validation_epoch_loss'=1.2716\n",
      "100%|██████████| 12/12 [00:01<00:00, 11.62it/s]\n",
      "2022-02-28 14:17:47 INFO:101] Epoch[28] Elapsed time 1.035 seconds\n",
      "2022-02-28 14:17:47 INFO:104] Epoch[28] Evaluation metric 'epoch_loss'=1.3052\n",
      "2022-02-28 14:17:48 INFO:104] Epoch[28] Evaluation metric 'validation_epoch_loss'=1.2655\n",
      "100%|██████████| 12/12 [00:01<00:00, 11.49it/s]\n",
      "2022-02-28 14:17:49 INFO:101] Epoch[29] Elapsed time 1.046 seconds\n",
      "2022-02-28 14:17:49 INFO:104] Epoch[29] Evaluation metric 'epoch_loss'=1.3133\n",
      "2022-02-28 14:17:49 INFO:104] Epoch[29] Evaluation metric 'validation_epoch_loss'=1.2832\n",
      "100%|██████████| 12/12 [00:01<00:00, 11.55it/s]\n",
      "2022-02-28 14:17:50 INFO:101] Epoch[30] Elapsed time 1.040 seconds\n",
      "2022-02-28 14:17:50 INFO:104] Epoch[30] Evaluation metric 'epoch_loss'=1.3261\n",
      "2022-02-28 14:17:51 INFO:104] Epoch[30] Evaluation metric 'validation_epoch_loss'=1.3496\n",
      "100%|██████████| 12/12 [00:01<00:00, 11.64it/s]\n",
      "2022-02-28 14:17:52 INFO:101] Epoch[31] Elapsed time 1.033 seconds\n",
      "2022-02-28 14:17:52 INFO:104] Epoch[31] Evaluation metric 'epoch_loss'=1.3365\n",
      "2022-02-28 14:17:52 INFO:104] Epoch[31] Evaluation metric 'validation_epoch_loss'=1.3091\n",
      "100%|██████████| 12/12 [00:01<00:00, 11.59it/s]\n",
      "2022-02-28 14:17:53 INFO:101] Epoch[32] Elapsed time 1.037 seconds\n",
      "2022-02-28 14:17:53 INFO:104] Epoch[32] Evaluation metric 'epoch_loss'=1.3228\n",
      "2022-02-28 14:17:53 INFO:104] Epoch[32] Evaluation metric 'validation_epoch_loss'=1.2592\n",
      "100%|██████████| 12/12 [00:01<00:00, 11.48it/s]\n",
      "2022-02-28 14:17:54 INFO:101] Epoch[33] Elapsed time 1.046 seconds\n",
      "2022-02-28 14:17:54 INFO:104] Epoch[33] Evaluation metric 'epoch_loss'=1.2896\n",
      "2022-02-28 14:17:55 INFO:104] Epoch[33] Evaluation metric 'validation_epoch_loss'=1.2685\n",
      "100%|██████████| 12/12 [00:01<00:00, 11.52it/s]\n",
      "2022-02-28 14:17:56 INFO:101] Epoch[34] Elapsed time 1.043 seconds\n",
      "2022-02-28 14:17:56 INFO:104] Epoch[34] Evaluation metric 'epoch_loss'=1.2865\n",
      "2022-02-28 14:17:56 INFO:104] Epoch[34] Evaluation metric 'validation_epoch_loss'=1.2440\n",
      "100%|██████████| 12/12 [00:01<00:00, 11.62it/s]\n",
      "2022-02-28 14:17:57 INFO:101] Epoch[35] Elapsed time 1.034 seconds\n",
      "2022-02-28 14:17:57 INFO:104] Epoch[35] Evaluation metric 'epoch_loss'=1.3066\n",
      "2022-02-28 14:17:58 INFO:104] Epoch[35] Evaluation metric 'validation_epoch_loss'=1.3341\n",
      "100%|██████████| 12/12 [00:01<00:00, 11.51it/s]\n",
      "2022-02-28 14:17:59 INFO:101] Epoch[36] Elapsed time 1.044 seconds\n",
      "2022-02-28 14:17:59 INFO:104] Epoch[36] Evaluation metric 'epoch_loss'=1.3650\n",
      "2022-02-28 14:17:59 INFO:104] Epoch[36] Evaluation metric 'validation_epoch_loss'=1.3947\n",
      "100%|██████████| 12/12 [00:01<00:00, 11.59it/s]\n",
      "2022-02-28 14:18:00 INFO:101] Epoch[37] Elapsed time 1.037 seconds\n",
      "2022-02-28 14:18:00 INFO:104] Epoch[37] Evaluation metric 'epoch_loss'=1.3830\n",
      "2022-02-28 14:18:00 INFO:104] Epoch[37] Evaluation metric 'validation_epoch_loss'=1.2819\n",
      "100%|██████████| 12/12 [00:01<00:00, 11.55it/s]\n",
      "2022-02-28 14:18:01 INFO:101] Epoch[38] Elapsed time 1.040 seconds\n",
      "2022-02-28 14:18:01 INFO:104] Epoch[38] Evaluation metric 'epoch_loss'=1.3269\n",
      "2022-02-28 14:18:02 INFO:104] Epoch[38] Evaluation metric 'validation_epoch_loss'=1.2818\n",
      "100%|██████████| 12/12 [00:01<00:00, 11.63it/s]\n",
      "2022-02-28 14:18:03 INFO:101] Epoch[39] Elapsed time 1.033 seconds\n",
      "2022-02-28 14:18:03 INFO:104] Epoch[39] Evaluation metric 'epoch_loss'=1.2944\n",
      "2022-02-28 14:18:03 INFO:104] Epoch[39] Evaluation metric 'validation_epoch_loss'=1.2604\n",
      "100%|██████████| 12/12 [00:01<00:00, 11.60it/s]\n",
      "2022-02-28 14:18:04 INFO:101] Epoch[40] Elapsed time 1.036 seconds\n",
      "2022-02-28 14:18:04 INFO:104] Epoch[40] Evaluation metric 'epoch_loss'=1.3178\n",
      "2022-02-28 14:18:04 INFO:104] Epoch[40] Evaluation metric 'validation_epoch_loss'=1.3704\n",
      "100%|██████████| 12/12 [00:01<00:00, 11.54it/s]\n",
      "2022-02-28 14:18:05 INFO:101] Epoch[41] Elapsed time 1.041 seconds\n",
      "2022-02-28 14:18:05 INFO:104] Epoch[41] Evaluation metric 'epoch_loss'=1.3394\n",
      "2022-02-28 14:18:06 INFO:104] Epoch[41] Evaluation metric 'validation_epoch_loss'=1.2548\n",
      "100%|██████████| 12/12 [00:01<00:00, 11.45it/s]\n",
      "2022-02-28 14:18:07 INFO:101] Epoch[42] Elapsed time 1.049 seconds\n",
      "2022-02-28 14:18:07 INFO:104] Epoch[42] Evaluation metric 'epoch_loss'=1.2747\n",
      "2022-02-28 14:18:07 INFO:104] Epoch[42] Evaluation metric 'validation_epoch_loss'=1.2134\n",
      "100%|██████████| 12/12 [00:01<00:00, 11.44it/s]\n",
      "2022-02-28 14:18:08 INFO:101] Epoch[43] Elapsed time 1.050 seconds\n",
      "2022-02-28 14:18:08 INFO:104] Epoch[43] Evaluation metric 'epoch_loss'=1.2326\n",
      "2022-02-28 14:18:09 INFO:104] Epoch[43] Evaluation metric 'validation_epoch_loss'=1.2104\n",
      "100%|██████████| 12/12 [00:01<00:00, 11.30it/s]\n",
      "2022-02-28 14:18:10 INFO:101] Epoch[44] Elapsed time 1.063 seconds\n",
      "2022-02-28 14:18:10 INFO:104] Epoch[44] Evaluation metric 'epoch_loss'=1.2273\n",
      "2022-02-28 14:18:10 INFO:104] Epoch[44] Evaluation metric 'validation_epoch_loss'=1.1824\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss is: 1.7111382285753887\n"
     ]
    }
   ],
   "source": [
    "tr_loader, va_loader, full_loader, te_loader = get_dataloader(path = data_dir, \n",
    "                                                              window_size = window_size,\n",
    "                                                              strides = strides,\n",
    "                                                              batch_size = 2,\n",
    "                                                              mask_rate = 0,\n",
    "                                                              simulate = False,\n",
    "                                                              )\n",
    "\n",
    "\n",
    "trainer.epochs = epoch_info['epoch_no']\n",
    "trainer.early_stopping_patience = 100\n",
    "model, epoch_info_retrain = trainer(model, loss_func, full_loader, full_loader)\n",
    "\n",
    "test_loss, test_time = model_inference(model, te_loader)\n",
    "print('Test loss is:', test_loss)\n",
    "\n",
    "# 1.6136637772481466"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ConvLSTM(input_dim = input_size, hidden_dim = 1, kernel_size = (3,3), num_layers = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_batch = next(iter(tr_loader))\n",
    "\n",
    "input_tensor = tmp_batch['x']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 50, 352, 9])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp_batch['x'].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_full = np.load(data_dir + '/processed_features.npy') #standardized inputs\n",
    "x_raw_full = np.load(data_dir + '/features.npy') #raw inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.70143216, -1.69774938, -1.50129899,  0.07368764, -0.4396963 ,\n",
       "        1.06835533,  0.34256487, -0.38742411, -0.17148342])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_full[0,-2,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.70143216,  1.07408634, -1.50129899,  0.07368764, -0.4396963 ,\n",
       "        1.06835533,  0.34256487, -0.38742411, -0.17148342])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_full[40,-2,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
